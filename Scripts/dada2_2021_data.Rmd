---
title: "2021 DADA2"
author: "Victor Trandafir"
date: "11/22/2021"
output: github_document
---

This script processes trimmed (w/o primers) sequences through the [DADA2 (1.18) pipeline](https://benjjneb.github.io/dada2/tutorial.html). 

# Install and load DADA2 and Shortread from Bioconductor

uncomment (remove the #'s preceding each line of code) the following code to install the required packages. This may take a few minutes.
```{r}
#if (!requireNamespace("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
#BiocManager::install("dada2", version = '3.14')
#BiocManager::install("ShortRead")
```

# Load packages
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(dada2)
library(ShortRead)
```

# Import data

You will need to download your sequences from the [shared google drive](https://drive.google.com/drive/folders/1kmLhML2Sanv0gE8RnxpcyNEcrhan-Q2h?usp=sharing) to a folder in your repository. Because this folder will then contain a lot of data (over the limit for pushing to the GitHub server), you'll want to have your commits ignore the addition and any changes to that folder. THIS IS IMPORTANT. You can modify the .gitignore file in your repository by:

-clicking on the .gitignore file to open it in R Studio if you can see it in the "Files" tab of RStudio (the bottom right pane where you can navigate around inside your 144L repository)

OR by:  
- revealing hidden files (command + shift + . on a mac and in [windows](https://support.microsoft.com/en-us/help/4028316/windows-view-hidden-files-and-folders-in-windows-10), open *file explorer*, select *view > options > change folder* and  *search options*, select *view* and in *advanced setting*, select *show hidden files, folders, and drives* )
- open the .gitignore file (textedit on a mac and notepad in widows)
- add the name of the folder containing the fastq (sequence) files to the .gitignore file, save, and close

```{r}
#save the path to the directory with a COPY of your unzipped fastq files that you WILL work with. MAKE SURE YOU HAVE ANOTHER DIRECTORY WITH THE FILES THAT YOU WILL NEVER DIRECTLY WORK WITH. 

path <- "~/Documents/College/Fourth Year/EEMB 144L/Github/144l_students_2021/Input_Data/week9/Kelp_Remin_fastq_copy" 
#make sure there is no / at the end of the path
#also make sure there are no unzipped files in this directory

#store the names of fwd and rv files as lists
fnFs <- list.files(path, pattern = "_R1_001.fastq", full.names = TRUE)
fnRs <- list.files(path, pattern = "_R2_001.fastq", full.names = TRUE)
```

# Retrieve orientation of primers

This part stores all the orientations of the V4 514F-Y and 806RB primers ( [see Apprill et al., 2015](http://www.int-res.com/articles/ame_oa/a075p129.pdf)), so that we can look for them when we want to trim them out. 

```{r}
FWD = "GTGYCAGCMGCCGCGGTAA"
REV = "GGACTACNVGGGTWTCTAAT"

#now store all orientations of fwd and rvs primers 
allOrients <- function(primer) {
  # Biostrings works w/ DNAString objects rather than character vectors
  require(Biostrings)
  dna <- DNAString(primer)
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
              RevComp = reverseComplement(dna))
  #Convert back to character vector
  return(sapply(orients, toString))
  }
  
#Store the fwd and rvs orientations separately 
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)

#view the orientations of the primers
FWD.orients
REV.orients
```

#search for Primers

```{r}
primerHits <- function(primer, fn) {
  #Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits >0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs[[1]]))
```

# Insepct read quality profiles 

You should look at least some of the quality profiles to assess the quality of the sequencing run. 

## Forward reads 

```{r fig.height=10, fig.width=12}
plotQualityProfile(fnFs[1:12])
```

# Reverse reads 

```{r fig.height=10, fig.width=12}
plotQualityProfile(fnRs[1:12])
```

# Filtering and Trimming 

```{r}
#Get the sample names
#define the basename of the fnFs as the first part of each fastq file name until "_L"
#apply this to all samples
sample.names <- sapply(strsplit(basename(fnFs), "_L"), `[`, 1)
sample.names
#created a "filtered" folder in the working directory as a place to put all the new filtered fastQ files. 
filt_path <- file.path(path, "filtered")
#add the appropriate designation string to any new files made that will be put into the "filtered" folder
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq"))
```

Below is the actual filtering step. We're using standard filtering parameters. 
1. dada2 generally advises trimming the last few nucleotides for weird sequencing errors that can pop up there. 
2. maxEE is the max number of expected errors (calculated from Q's) to allow in each read. This is a probability calculation. 
3. minQ is threshold Q - and read with a Q < minQ after truncating reads get discarded. This isn't that important for 16S. 

```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen = c(240, 150), maxN = 0, maxEE = c(2,2), truncQ = 2, rm.phix = TRUE, compress = TRUE)
#look at this output. This tells you how many reads were removed. 
out <- transform(out, percent_retained = ((reads.out/reads.in)*100)) #Add column to out with the percent retained for each read.
out
mean(out$percent_retained) #Calculate average retention for all reads.
```

# Learn the error rates

```{r}
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
```

This took ~20 seconds each on a 2021 MacBook Pro.

This dada2 algorithm makes use of a parametric error model (err) as every amplicon dataset has a different set of error rates. This is what dada2 is all about. The step creates the parameters for designating unique sequences. 

Each sequence has an x number of reads. dada2 uses the numbers of reads per sequence as well as the q-score to build this model. This algorithm assumes that your most abundant sequence is "real" (the true sequence for a particular organism). There is a very high probability that it is. 

What the algorithm does is look at each base pair of an individual sequence and calculate the probability that the base pair is an error based on the quality score of the read and the sequence of your most abundant read. It also does this for the second most abundant sequence, etc. etc. hence the message "convergence after x rounds" after running the algorithm. 

```{r echo= FALSE, warning = FALSE, message = FALSE, fig.height=10, fig.width=12, fig.align="center", warning= FALSE}
plotErrors(errF, nominalQ = TRUE)
```

```{r echo= FALSE, warning = FALSE, message = FALSE, fig.height=10, fig.width=12, fig.align="center", warning= FALSE}
plotErrors(errR, nominalQ = TRUE)
```

# Dereplication 

This is another big thing that dada2 does. It combines all identical sequences into one unique sequence, keeping track of the number of identical sequences. 

```{r}
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names 
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

# Infer sequence variants 

Apply the core dada2 sample inference algorithm to the dereplicated data. 

Infer the sequence variants in each sample, taking out the sequence variants that have excessive error rates. 

So here, we are applying the error models to the data. Before, the error models were run using a subset of the data (parameterizing). Now, we're using the parameters of the model and applying it to the whole data set to see which sequences are real and which are not. 

```{r}
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)
```

merge the overlapping reads -> this will also decrease the number of sequence variants. 
If you above had hits of the reverse complement in the FWD.ReverseReads and the REV.ForwardReads, you can trim here by adding trimOverhang = T. 

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE, trimOverhang = T)
```

inspect the merged data frame from the first sample. this will output a table. the numbers in the forward and reverse columns tell where those sequences are in the dadaFs and dadaRs files.

```{r}
head(mergers[[1]])
```

save the unassigned merged reads

```{r}
#tutorial says week 5/6, just save in Input & Output data for Week 7 (we are on a different schedule this year than previous)
saveRDS(mergers, "~/Documents/College/Fourth Year/EEMB 144L/Github/144l_students_2021/Output_Data/week9/dada_merged.rds")
saveRDS(mergers, "~/Documents/College/Fourth Year/EEMB 144L/Github/144l_students_2021/Input_Data/week9/dada_merged.rds")
```

Construct a sequence table of our samples that is analogous to the "OTU table" produced by classical methods

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab) # samples by unique sequence
```

check the distribution of sequence lengths 

```{r}
table(nchar(getSequences(seqtab)))
```

# Remove the Chimeras 

in PCR, two or more biological sequences can attach to each other and then polymerase builds a non-biological sequence. weird. These are artefacts that need to be removed. 

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, verbose = TRUE)
dim(seqtab.nochim)
```

check the proportion of sequences that are not chimeras
```{r}
sum(seqtab.nochim)/sum(seqtab)
```

# Assign taxonomy using a reference database

here we are referencing the Silva database

```{r}
  taxa <- assignTaxonomy(seqtab.nochim, "~/Documents/College/Fourth Year/EEMB 144L/Github/144l_students_2021/Input_Data/week9/Reference_Database/silva_nr_v138_train_set.fa", multithread = TRUE)

```

This took ~90s to complete on a 2021 MacBook Pro

create a table out of the taxa data (one with the sequences and taxonomic assignments, one with just all the taxa)

these are the tables you want to save!! 

```{r}
saveRDS(t(seqtab.nochim), "~/Documents/College/Fourth Year/EEMB 144L/Github/144l_students_2021/Output_Data/week9/seqtab-nochimtaxa.rds")
saveRDS(taxa, "~/Documents/College/Fourth Year/EEMB 144L/Github/144l_students_2021/Output_Data/week9/taxa.rds")

#save in both Output and Input data folders for week7

saveRDS(t(seqtab.nochim), "~/Documents/College/Fourth Year/EEMB 144L/Github/144l_students_2021/Input_Data/week9/seqtab-nochimtaxa.rds")
saveRDS(taxa, "~/Documents/College/Fourth Year/EEMB 144L/Github/144l_students_2021/Input_Data/week9/taxa.rds")
        
```
